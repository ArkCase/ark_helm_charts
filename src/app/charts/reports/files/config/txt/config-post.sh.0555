#!/bin/bash
set -euo pipefail
. /.functions

set_as_boolean DEBUG

cleanup()
{
	[ -v RUN_MARKER ] || RUN_MARKER=""
	[ -z "${RUN_MARKER}" ] || rm -rf "${RUN_MARKER}" &>/dev/null
}

RUN_MARKER="${PENTAHO_HOME}/.initRan"
trap cleanup EXIT

set_or_default BASE_DIR "/app"
set_or_default INIT_DIR "${BASE_DIR}/init"
set_or_default DATA_DIR "${BASE_DIR}/data"
set_or_default DWHS_DIR "${DATA_DIR}/dw"
set_or_default LOGS_DIR "${BASE_DIR}/logs"

set_or_default ADMIN_PORT "8443"

redirect_logs "${LOGS_DIR}/config-post.log"

# By default, wait up to 900 seconds if not told otherwise
set_or_default INIT_POLL_SLEEP
[[ "${INIT_POLL_SLEEP}" =~ ^[1-9][0-9]*$ ]] || INIT_POLL_SLEEP=2
set_or_default INIT_MAX_WAIT
[[ "${INIT_MAX_WAIT}" =~ ^[1-9][0-9]*$ ]] || INIT_MAX_WAIT=900

set_or_default ADMIN_URL "https://localhost:${ADMIN_PORT}/pentaho/"
poll_url "${ADMIN_URL}" "${INIT_MAX_WAIT}" "${INIT_POLL_SLEEP}" || fail "Cannot continue configuration if Pentaho is not online"

is_file "${RUN_MARKER}" || quit "Run marker not found!"

# First things first: go through every config directory in the DW reports areas,
# and render all the templates. THEN, add all the connections and schemas, one by one
JOBS=()
while read JOB ; do
	DPL="${JOB}/deploy"
	doing "Deploying the DW reports at [${JOB}]..."

	INCOMPLETE="false"
	if [ -d "${DPL}" ] ; then
		(
			set -euo pipefail
			source <(encrypt-passwords kettle) || fail "Failed to encrypt the passwords"
			while read T ; do
				# Remove the ".tpl" extension...
				F="${T%.*}"
				doing "Rendering [${F}]..."
				render-template < "${T}" > "${F}"
			done < <(find "${DPL}" -type f -iname '*.tpl' | sort)
		)

		# Find any connections (i.e. connection-*.json)
		while read CONNECTION ; do
			if ! add-pdi-connection "${CONNECTION}" ; then
				err "\tconnection installation failed!"
				INCOMPLETE="true"
				continue
			fi

			# This is a small hack ... we've found intermittent ConcurrentModificationExceptions
			# puke all over report installation, so we're going to delay everything for a few
			# seconds to let this datasource creation operation settle down a little bit
			#
			# Yes... waits SUCK ... but until we find a more robust means of checking
			# if the DataSource is ready to be consumed, this should help for now
			sleep 5 || true

		done < <(find "${DPL}" -type f -iname 'connection-*.json' | sort)

		# Find any Mondrian schemata (i.e. schema-*.xml)
		while read SCHEMA ; do
			if ! install-mondrian-schema "${SCHEMA}" ; then
				err "\tschema installation failed!"
				INCOMPLETE="true"
				continue
			fi
		done < <(find "${DPL}" -type f -iname 'schema-*.xml' | sort)
	fi

	if ${INCOMPLETE} ; then
		err "The artifacts for the job at [${JOB}] are incomplete, will not launch the job"
		continue
	fi

	# Find the first *.kjb (case-insensitive) file in that folder, and run that
	JOB="$(find "${JOB}" -mindepth 1 -maxdepth 1 -type f -iname '*.kjb' | sort | head -1)"
	if [ -n "${JOB}" ] ; then
		doing "\tAdding a PDI bootup job: [${JOB}]"
		JOBS+=("${JOB}")
	fi

done < <(find "${DWHS_DIR}" -mindepth 1 -maxdepth 1 -type d | sort)
JOBS_COUNT=${#JOBS[@]}

if [ ${JOBS_COUNT} -gt 0 ] ; then
	# Before we can run the dataminer first time, we MUST wait for
	# ArkCase to come up ... if it doesn't, we're screwed
	set_or_default CORE_URL "https://core:8443/arkcase/"
	say "Found ${JOBS_COUNT} dataminers:"
	for J in "${JOBS[@]}" ; do
		say "\t${J}"
	done
	running "Launching the background poller for [${CORE_URL}]..."
	coproc { poll_url "${CORE_URL}" "${INIT_MAX_WAIT}" "${INIT_POLL_SLEEP}" ; }
else
	warn "No jobs found, will not wait for ArkCase to boot up"
fi

# Install the reports ... this *has* to happen last b/c the PDI stuff
# won't install cleanly if the rest of its dependencies aren't already covered
#
# We also run this in parallel with waiting for ArkCase to come up, b/c
# we try to be efficient with time :D
doing "Deploying reports"
install-reports
ok "Reports deployed"

# If we have to rejoin with the ArkCase poller, do so...
if [ -v COPROC_PID ] ; then
	waiting "Joining the background poller for [${CORE_URL}]..."
	wait ${COPROC_PID} || fail "Failed to wait for ArkCase to be online"
fi

# Leave this here ... if there are no jobs, nothing will happen...
for JOB in "${JOBS[@]}" ; do
	JOB_LOG="${JOB%.*}.log"
	doing "Launching the first-time PDI job [${JOB}]..."
	if run-kjb "${JOB}" < /dev/null &> "${JOB_LOG}" ; then
		ok "Job completed successfully"
	else
		err "Errors detected, please review the log at [${JOB_LOG}]"
	fi
done

ok "Post-configuration completed"
