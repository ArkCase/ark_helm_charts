# Default values for ark-core.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This can be set to "false" by parent charts. If not defined, it will be taken as true
#enabled: true

# licenses:
#   pdfnet: "some-license-string"

#
# This should group all application-specific configurations
#
configuration:
  proxy:
    reports: "https://reports:8443"
    weather: "https://api.openweathermap.org/data/2.5"

resources:
  development:
    arkcase:
      limits: "3Gi,2"

# Please leave the `replicaCount` to 1 for the time being. Clustering is not
# supported yet.
replicaCount: 1

platform: ""

licenses:
  - "pdftron"

image:
  # pullPolicy: IfNotPresent
  pullSecrets:
    - name: aws-arkcase-pull

  registry: "public.ecr.aws"

  # Overrides the image tag whose default is the chart appVersion.
  repository: "arkcase/core"
  # tag: "3.0.0"

  deployer:
    repository: "arkcase/deployer"
    # tag: 1.1.1-01

  haproxy:
    repository: "arkcase/haproxy"
    tag: "2.6"

  # setperm:
  #   registry: "alternate-registry"
  #   repository: "arkcase/setperm"
  #   tag: "latest"

  # seed-content:
  #   registry: "alternate-registry"
  #   repository: "arkcase/nettest"
  #   tag: "latest"


# You generally shouldn't mess with these. These exist to support some name-generation templates
nameOverride: ""
fullnameOverride: ""

# Custom annotations to apply throughout
annotations:

# Custom labels to apply throughout
labels:

updateStrategy: RollingUpdate
rollingUpdatePartition: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations:
  # Annotations to add to the service account
  labels:
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

securityContext:
  # You must specify an `fsGroup` to allow ActiveMQ write access to mounted
  # volumes (as long as ActiveMQ runs as a non-root user, which should be the
  # case). Any random number will do.
  # fsGroup: &fsGroup 1997
  # fsGroupChangePolicy: OnRootMismatch

  arkcase:
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: &runAsUser 1997
    # runAsGroup: *fsGroup

#autoscaling:
  #enabled: false
  #minReplicas: 1
  #maxReplicas: 100
  #targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector:

tolerations:

affinity:

persistence:
  # If set to `false`, an ephemeral volume will be used instead and all other `persistence.*` parameters
  # are ignored.
  enabled: true

  # Set the default capacity for volumes in case none is assigned explicitly
  # and the default value is insufficient
  volumeSize:
    conf: "4Gi"
    home: "8Gi"
    init: "4Gi"
    logs: "2Gi"
    wars: "4Gi"
    temp: "32Gi"
    prtl: "16Gi"
    fsrv: "32Gi"

#  The volumes for persistent data
  volumes:
# The volume where the data will be stored (RTFM for more info on how to
# properly declare volumes)

service:
  canDebug: true

  arkcase:
    ports:
      - name: https
        protocol: TCP
        port: &https 8443

    probes: &arkcaseProbes
      enabled: true
      spec: &baseProbeThreshold
        initialDelaySeconds: 0
        periodSeconds: 10
        timeoutSeconds: 10
        successThreshold: 1
        failureThreshold: 1
      startup:
        exec:
          command: [ "/usr/local/bin/tomcat-live", "STARTUP" ]
        failureThreshold: 90
      liveness:
        exec:
          command: [ "/usr/local/bin/tomcat-live", "LIVENESS" ]
        # For our liveness probe, we allow up to 6 probes to fail in case
        # the pod is too resource-starved to respond to probes. This is a
        # bad thing anyway, but we have yet to reach a point where we can
        # safely pare down this number to a more reasonable count.
        failureThreshold: 6
      readiness:
        httpGet:
          scheme: HTTPS
          path: "/arkcase/login"
          port: *https

  clusteredArkcase:
    ports:
      - name: clustered-https
        protocol: TCP
        port: &clustered 4443
      - name: hazelcast
        protocol: TCP
        port: 5710

    probes:
      <<: *arkcaseProbes
      readiness:
        httpGet:
          scheme: HTTPS
          path: "/arkcase/login"
          port: *clustered

  haproxy:
    ports:
      - name: healthz
        protocol: TCP
        port: &healthz 18443
    probes:
      enabled: true
      spec:
        <<: *baseProbeThreshold
      startup: &haproxyProbe
        httpGet:
          scheme: HTTPS
          port: *healthz
          path: "/healthz"
        failureThreshold: 90
      liveness:
        <<: *haproxyProbe
        failureThreshold: 3
      readiness:
        <<: *haproxyProbe
        failureThreshold: 1
